{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2>Sparse Googlenet</h2>\nSubmission by: <a href=\"https://github.com/id56\">Mashrur Mahmud</a><br>\nModel Overview: Weight Pruned Googlenet, trained with Cutout and Label Smoothing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torchvision import datasets,transforms, models\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport os\nimport sys\nimport time\nimport math","execution_count":45,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Hyperparameter Settings</h3>\nI trained my model under the following hyperparameters:\n<ul>\n    <li>batch size = 128</li>\n    <li>maximum epochs = 200 </li>\n    <li>learning rate = 0.1</li>\n    <li>warmup training epochs = 2</li>\n    <li>train scheduling milestones = 60, 120, 160, 180, 200</li>\n    <li>learning rate decay factor = 0.1 (reduced to a tenth, every milestone)</li>\n    <li>cutout length = 8 (8x8 pixel square)\n    <li>random seed = 56 (for reproducibility)</li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize = 128\nmilestones = [60,120,160,180,200]\nw = 2\ntorch.manual_seed(56)","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"<torch._C.Generator at 0x7f7e09298030>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Set the directory of the final checkpoint and the location of CIFAR_100(where it is downloaded/to be downloaded)\n### My directories are set in Kaggle style since I trained them on the kaggle cloud.\n\nfinal_path = '../input/sparsegnet100/sparsegnetv5_200ep.pth'\n\ndownload = False\ndata_dir='../input/cifar100/cifar-100-python/'","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Loading and Preprocessing CIFAR100:</h3>\n<p>I want to normalize the data as a part of my preprocessing. The code cell below calculates the mean and standard deviation along each color channel.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#def compute_mean_std(dataset):\n    \n#    data_r = np.dstack([np.array(im)[:, :, 0]/255 for im,label in dataset])\n#    data_g = np.dstack([np.array(im)[:, :, 1]/255 for im,label in dataset])\n#    data_b = np.dstack([np.array(im)[:, :, 2]/255 for im,label in dataset])\n    \n#    mean = [np.asscalar(np.mean(data_r)), np.asscalar(np.mean(data_g)), np.asscalar(np.mean(data_b))]\n#    std = [np.asscalar(np.std(data_r)), np.asscalar(np.std(data_g)), np.asscalar(np.std(data_b))]\n    \n#    return mean,std","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cifar_norm_train=datasets.CIFAR100(data_dir, train=True, transform=None, target_transform=None, download=False)\n#cifar_norm_test=datasets.CIFAR100(data_dir, train=False, transform=None, target_transform=None, download=False)\n#train_mean,train_std = compute_mean_std(cifar_norm_train)\n#test_mean, test_std = compute_mean_std(cifar_norm_test)\n#print(train_mean,train_std)\n#print(test_mean,test_std)","execution_count":49,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Uncomment the above cells if you wish to determine normalization values yourself. I saved the resultant values below so that I don't have to run the above computation each time I use this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mean,train_std = [0.5070751592371341, 0.48654887331495067, 0.4409178433670344],[0.2673342858792403, 0.2564384629170882, 0.27615047132568393]\ntest_mean, test_std = [0.508796412760417, 0.48739301317401906, 0.4419422112438727],[0.2682515741720801, 0.25736373644781246, 0.2770957707973041]","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cutout implementation from [https://github.com/uoguelph-mlrg/Cutout](https://github.com/uoguelph-mlrg/Cutout)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cutout(object):\n    \"\"\"Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    \"\"\"\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        \"\"\"\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img = img * mask\n\n        return img","execution_count":51,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing steps:\n<ul><li>padding with 4 zeros on each side, resulting in a 40 by 40 image</li>\n    <li>taking 32 by 32 random crops</li>\n    <li>random horizontal flipping</li>\n    <li>applying cutout (8x8 px) </li>\n    <li>normalization by values found above</li></ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transforms = transforms.Compose([transforms.Pad(4,fill=0),\n                                     transforms.RandomResizedCrop(32),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(train_mean,train_std),\n                                     Cutout(n_holes=1, length=8)])\n\ntest_transforms = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(test_mean,test_std)])\n\n\ncifar_train=datasets.CIFAR100(data_dir, train=True, transform=train_transforms, target_transform=None, download=download)\ncifar_test=datasets.CIFAR100(data_dir, train=False, transform=test_transforms, target_transform=None, download=download)\n\ntrainloader = torch.utils.data.DataLoader(cifar_train, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True)\ntestloader = torch.utils.data.DataLoader(cifar_test, batch_size=batchsize, shuffle=True, num_workers=4, pin_memory=True)","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Visualizing CIFAR</h3>\n<p>Just to see what's actually in there.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for images,labels in testloader:\n    break\n_, axarr = plt.subplots(4,4)\nfor i in range(4):\n    for j in range(4):\n        axarr[i,j].axis('off')\n        axarr[i,j].imshow(images[i*5+j].T)","execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 16 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"<h3>Pruning</h3>\nWe'll prune two types of layers, according to Han et al. ('15) - the fully connected layers and convolutional layers. In googlenet, there is a single fc layer, and many conv layers as part of the inception module."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MaskedConv2d(Module):\n    r\"\"\"Applies a masked 2D convolution over an input signal composed of several input planes.\n    \n    In the simplest case, the output value of the layer with input size (N,C_in,H,W) and output\n    (N,C_out,H_out,W_out) can be precisely described as:\n    \n    out(N_i,C_out_j) = bias(C_out_j) + ∑[k:0,C_in-1](weight(C_out_j,k)*input(N_i,k))\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, padding = 0, bias = True):\n        super(MaskedConv2d, self).__init__()\n        \n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.weight = Parameter(torch.Tensor(out_channels, in_channels, kernel_size,kernel_size))\n        self.input_shape = 0\n        # Initialize the mask with 1\n        self.mask = Parameter(torch.ones([out_channels, in_channels, kernel_size,kernel_size]), requires_grad=False)\n        \n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1)*self.weight.size(2)*self.weight.size(3))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input):\n        self.input_shape = input.shape[2]\n        return F.conv2d(input, self.weight * self.mask, self.bias, padding = self.padding)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n            + 'in_channels=' + str(self.in_channels) \\\n            + ', out_channels=' + str(self.out_channels) \\\n            + ', kernel_size=' + str(self.kernel_size) \\\n            + ', bias=' + str(self.bias is not None) + ')'\n\n    def prune(self, threshold):\n        weight_dev = self.weight.device\n        mask_dev = self.mask.device\n        # Convert Tensors to numpy and calculate\n        tensor = self.weight.data.cpu().numpy()\n        mask = self.mask.data.cpu().numpy()\n        new_mask = np.where(abs(tensor) < threshold, 0, mask)\n        # Apply new weight and mask\n        self.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n        self.mask.data = torch.from_numpy(new_mask).to(mask_dev)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MaskedLinear(Module):\n    r\"\"\"Applies a masked linear transformation to the incoming data: y = (A * M)x + b`\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        bias: If set to False, the layer will not learn an additive bias.\n            Default: ``True``\n    Shape:\n        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n          additional dimensions\n        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n          are the same shape as the input.\n    Attributes:\n        weight: the learnable weights of the module of shape\n            (out_features x in_features)\n        bias:   the learnable bias of the module of shape (out_features)\n        mask: the unlearnable mask for the weight.\n            It has the same shape as weight (out_features x in_features)\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(MaskedLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(out_features, in_features))\n        # Initialize the mask with 1\n        self.mask = Parameter(torch.ones([out_features, in_features]), requires_grad=False)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input):\n        return F.linear(input, self.weight * self.mask, self.bias)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n            + 'in_features=' + str(self.in_features) \\\n            + ', out_features=' + str(self.out_features) \\\n            + ', bias=' + str(self.bias is not None) + ')'\n\n    def prune(self, threshold):\n        weight_dev = self.weight.device\n        mask_dev = self.mask.device\n        # Convert Tensors to numpy and calculate\n        tensor = self.weight.data.cpu().numpy()\n        mask = self.mask.data.cpu().numpy()\n        new_mask = np.where(abs(tensor) < threshold, 0, mask)\n        # Apply new weight and mask\n        self.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n        self.mask.data = torch.from_numpy(new_mask).to(mask_dev)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" def prune_by_std(layer, s=0.25):\n    \"\"\"\n    s is a quality parameter that is multiplied with the standard dev of a layer to determine threshold.\n\n    \"\"\"\n    for name, module in net.named_modules():\n        if layer in str(type(module)):\n            threshold = np.std(module.weight.data.cpu().numpy()) * s\n            print(f'Pruning with threshold : {threshold} for layer {name}')\n            module.prune(threshold)","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> The Network</h3>\nBaseline googlenet from <a href=\"https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/googlenet.py\">this repository.</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Inception(nn.Module):\n    def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj, mask=False):\n        super().__init__()\n        \n        Conv2d = MaskedConv2d if mask else nn.Conv2d\n        \n        #1x1conv branch\n        self.b1 = nn.Sequential(\n            Conv2d(input_channels, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(inplace=True)\n        )\n\n        #1x1conv -> 3x3conv branch\n        self.b2 = nn.Sequential(\n            Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n            nn.BatchNorm2d(n3x3_reduce),\n            nn.ReLU(inplace=True),\n            Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(inplace=True)\n        )\n\n        #1x1conv -> 5x5conv branch\n        #we use 2 3x3 conv filters stacked instead\n        #of 1 5x5 filters to obtain the same receptive\n        #field with fewer parameters\n        self.b3 = nn.Sequential(\n            Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n            nn.BatchNorm2d(n5x5_reduce),\n            nn.ReLU(inplace=True),\n            Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5, n5x5),\n            nn.ReLU(inplace=True),\n            Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(inplace=True)\n        )\n\n        #3x3pooling -> 1x1conv\n        #same conv\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            Conv2d(input_channels, pool_proj, kernel_size=1),\n            nn.BatchNorm2d(pool_proj),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n\n\nclass GoogleNet(nn.Module):\n\n    def __init__(self, num_class=100, mask = False):\n        super().__init__()\n        linear = MaskedLinear if mask else nn.Linear\n        Conv2d = MaskedConv2d if mask else nn.Conv2d\n        \n        self.prelayer = nn.Sequential(\n            Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n\n        #although we only use 1 conv layer as prelayer,\n        #we still use name a3, b3.......\n        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32, mask = mask)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64, mask = mask)\n\n        #\"\"\"In general, an Inception network is a network consisting of\n        #modules of the above type stacked upon each other, with occasional \n        #max-pooling layers with stride 2 to halve the resolution of the \n        #grid\"\"\"\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64, mask = mask)\n        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64, mask = mask)\n        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64, mask = mask)\n        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64, mask = mask)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128, mask = mask)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128, mask = mask)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128, mask = mask)\n\n        #input feature size: 8*8*1024\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout2d(p=0.4)\n        self.linear = linear(1024, num_class)\n    \n    def forward(self, x):\n        output = self.prelayer(x)\n        output = self.a3(output)\n        output = self.b3(output)\n        \n        output = self.maxpool(output)\n\n        output = self.a4(output)\n        output = self.b4(output)\n        output = self.c4(output)\n        output = self.d4(output)\n        output = self.e4(output)\n\n        output = self.maxpool(output)\n\n        output = self.a5(output)\n        output = self.b5(output)\n\n        #\"\"\"It was found that a move from fully connected layers to\n        #average pooling improved the top-1 accuracy by about 0.6%, \n        #however the use of dropout remained essential even after \n        #removing the fully connected layers.\"\"\"\n        \n        output = self.avgpool(output)\n        output = self.dropout(output)\n        output = output.view(output.size()[0], -1)\n        output = self.linear(output)\n\n        return output\n\ndef googlenet(mask=False):\n    return GoogleNet(mask=mask)","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Checking the baseline</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_nonzeros(model):\n    #param_bits = 32\n    nonzero = total = 0\n    for name, p in model.named_parameters():\n        if 'mask' in name:\n            continue\n        tensor = p.data.cpu().numpy()\n        nz_count = (np.count_nonzero(tensor))\n        total_params = np.prod(tensor.shape)\n        nonzero += nz_count\n        total += total_params\n        sparsity = 1-nz_count/total_params\n        print(f'{name:20} | total = {total_params/2:7} | params = {nz_count/2:7} | sparsity = {sparsity:6.2f} | shape = {tensor.shape}')\n    print('Total:',total,'Alive:',nonzero)\n    return(total)\n\n","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = googlenet(mask = True)\nprint_nonzeros(net)","execution_count":59,"outputs":[{"output_type":"stream","text":"prelayer.0.weight    | total =  2592.0 | params =  2592.0 | sparsity =   0.00 | shape = (192, 3, 3, 3)\nprelayer.0.bias      | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\nprelayer.1.weight    | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\nprelayer.1.bias      | total =    96.0 | params =     0.0 | sparsity =   1.00 | shape = (192,)\na3.b1.0.weight       | total =  6144.0 | params =  6144.0 | sparsity =   0.00 | shape = (64, 192, 1, 1)\na3.b1.0.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\na3.b1.1.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\na3.b1.1.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\na3.b2.0.weight       | total =  9216.0 | params =  9216.0 | sparsity =   0.00 | shape = (96, 192, 1, 1)\na3.b2.0.bias         | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\na3.b2.1.weight       | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\na3.b2.1.bias         | total =    48.0 | params =     0.0 | sparsity =   1.00 | shape = (96,)\na3.b2.3.weight       | total = 55296.0 | params = 55296.0 | sparsity =   0.00 | shape = (128, 96, 3, 3)\na3.b2.3.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na3.b2.4.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na3.b2.4.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\na3.b3.0.weight       | total =  1536.0 | params =  1536.0 | sparsity =   0.00 | shape = (16, 192, 1, 1)\na3.b3.0.bias         | total =     8.0 | params =     8.0 | sparsity =   0.00 | shape = (16,)\na3.b3.1.weight       | total =     8.0 | params =     8.0 | sparsity =   0.00 | shape = (16,)\na3.b3.1.bias         | total =     8.0 | params =     0.0 | sparsity =   1.00 | shape = (16,)\na3.b3.3.weight       | total =  2304.0 | params =  2304.0 | sparsity =   0.00 | shape = (32, 16, 3, 3)\na3.b3.3.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na3.b3.4.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na3.b3.4.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\na3.b3.6.weight       | total =  4608.0 | params =  4608.0 | sparsity =   0.00 | shape = (32, 32, 3, 3)\na3.b3.6.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na3.b3.7.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na3.b3.7.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\na3.b4.1.weight       | total =  3072.0 | params =  3072.0 | sparsity =   0.00 | shape = (32, 192, 1, 1)\na3.b4.1.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na3.b4.2.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na3.b4.2.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\nb3.b1.0.weight       | total = 16384.0 | params = 16384.0 | sparsity =   0.00 | shape = (128, 256, 1, 1)\nb3.b1.0.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb3.b1.1.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb3.b1.1.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nb3.b2.0.weight       | total = 16384.0 | params = 16384.0 | sparsity =   0.00 | shape = (128, 256, 1, 1)\nb3.b2.0.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb3.b2.1.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb3.b2.1.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nb3.b2.3.weight       | total = 110592.0 | params = 110592.0 | sparsity =   0.00 | shape = (192, 128, 3, 3)\nb3.b2.3.bias         | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\nb3.b2.4.weight       | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\nb3.b2.4.bias         | total =    96.0 | params =     0.0 | sparsity =   1.00 | shape = (192,)\nb3.b3.0.weight       | total =  4096.0 | params =  4096.0 | sparsity =   0.00 | shape = (32, 256, 1, 1)\nb3.b3.0.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\nb3.b3.1.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\nb3.b3.1.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\nb3.b3.3.weight       | total = 13824.0 | params = 13824.0 | sparsity =   0.00 | shape = (96, 32, 3, 3)\nb3.b3.3.bias         | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\nb3.b3.4.weight       | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\nb3.b3.4.bias         | total =    48.0 | params =     0.0 | sparsity =   1.00 | shape = (96,)\nb3.b3.6.weight       | total = 41472.0 | params = 41472.0 | sparsity =   0.00 | shape = (96, 96, 3, 3)\nb3.b3.6.bias         | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\nb3.b3.7.weight       | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\nb3.b3.7.bias         | total =    48.0 | params =     0.0 | sparsity =   1.00 | shape = (96,)\nb3.b4.1.weight       | total =  8192.0 | params =  8192.0 | sparsity =   0.00 | shape = (64, 256, 1, 1)\nb3.b4.1.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb3.b4.2.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb3.b4.2.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\na4.b1.0.weight       | total = 46080.0 | params = 46080.0 | sparsity =   0.00 | shape = (192, 480, 1, 1)\na4.b1.0.bias         | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\na4.b1.1.weight       | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\na4.b1.1.bias         | total =    96.0 | params =     0.0 | sparsity =   1.00 | shape = (192,)\na4.b2.0.weight       | total = 23040.0 | params = 23040.0 | sparsity =   0.00 | shape = (96, 480, 1, 1)\na4.b2.0.bias         | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\na4.b2.1.weight       | total =    48.0 | params =    48.0 | sparsity =   0.00 | shape = (96,)\na4.b2.1.bias         | total =    48.0 | params =     0.0 | sparsity =   1.00 | shape = (96,)\na4.b2.3.weight       | total = 89856.0 | params = 89856.0 | sparsity =   0.00 | shape = (208, 96, 3, 3)\na4.b2.3.bias         | total =   104.0 | params =   104.0 | sparsity =   0.00 | shape = (208,)\na4.b2.4.weight       | total =   104.0 | params =   104.0 | sparsity =   0.00 | shape = (208,)\na4.b2.4.bias         | total =   104.0 | params =     0.0 | sparsity =   1.00 | shape = (208,)\na4.b3.0.weight       | total =  3840.0 | params =  3840.0 | sparsity =   0.00 | shape = (16, 480, 1, 1)\na4.b3.0.bias         | total =     8.0 | params =     8.0 | sparsity =   0.00 | shape = (16,)\na4.b3.1.weight       | total =     8.0 | params =     8.0 | sparsity =   0.00 | shape = (16,)\na4.b3.1.bias         | total =     8.0 | params =     0.0 | sparsity =   1.00 | shape = (16,)\na4.b3.3.weight       | total =  3456.0 | params =  3456.0 | sparsity =   0.00 | shape = (48, 16, 3, 3)\na4.b3.3.bias         | total =    24.0 | params =    24.0 | sparsity =   0.00 | shape = (48,)\na4.b3.4.weight       | total =    24.0 | params =    24.0 | sparsity =   0.00 | shape = (48,)\na4.b3.4.bias         | total =    24.0 | params =     0.0 | sparsity =   1.00 | shape = (48,)\na4.b3.6.weight       | total = 10368.0 | params = 10368.0 | sparsity =   0.00 | shape = (48, 48, 3, 3)\na4.b3.6.bias         | total =    24.0 | params =    24.0 | sparsity =   0.00 | shape = (48,)\na4.b3.7.weight       | total =    24.0 | params =    24.0 | sparsity =   0.00 | shape = (48,)\na4.b3.7.bias         | total =    24.0 | params =     0.0 | sparsity =   1.00 | shape = (48,)\na4.b4.1.weight       | total = 15360.0 | params = 15360.0 | sparsity =   0.00 | shape = (64, 480, 1, 1)\na4.b4.1.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\na4.b4.2.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\na4.b4.2.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nb4.b1.0.weight       | total = 40960.0 | params = 40960.0 | sparsity =   0.00 | shape = (160, 512, 1, 1)\nb4.b1.0.bias         | total =    80.0 | params =    80.0 | sparsity =   0.00 | shape = (160,)\nb4.b1.1.weight       | total =    80.0 | params =    80.0 | sparsity =   0.00 | shape = (160,)\nb4.b1.1.bias         | total =    80.0 | params =     0.0 | sparsity =   1.00 | shape = (160,)\nb4.b2.0.weight       | total = 28672.0 | params = 28672.0 | sparsity =   0.00 | shape = (112, 512, 1, 1)\nb4.b2.0.bias         | total =    56.0 | params =    56.0 | sparsity =   0.00 | shape = (112,)\nb4.b2.1.weight       | total =    56.0 | params =    56.0 | sparsity =   0.00 | shape = (112,)\nb4.b2.1.bias         | total =    56.0 | params =     0.0 | sparsity =   1.00 | shape = (112,)\nb4.b2.3.weight       | total = 112896.0 | params = 112896.0 | sparsity =   0.00 | shape = (224, 112, 3, 3)\nb4.b2.3.bias         | total =   112.0 | params =   112.0 | sparsity =   0.00 | shape = (224,)\nb4.b2.4.weight       | total =   112.0 | params =   112.0 | sparsity =   0.00 | shape = (224,)\nb4.b2.4.bias         | total =   112.0 | params =     0.0 | sparsity =   1.00 | shape = (224,)\nb4.b3.0.weight       | total =  6144.0 | params =  6144.0 | sparsity =   0.00 | shape = (24, 512, 1, 1)\nb4.b3.0.bias         | total =    12.0 | params =    12.0 | sparsity =   0.00 | shape = (24,)\nb4.b3.1.weight       | total =    12.0 | params =    12.0 | sparsity =   0.00 | shape = (24,)\nb4.b3.1.bias         | total =    12.0 | params =     0.0 | sparsity =   1.00 | shape = (24,)\nb4.b3.3.weight       | total =  6912.0 | params =  6912.0 | sparsity =   0.00 | shape = (64, 24, 3, 3)\nb4.b3.3.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb4.b3.4.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb4.b3.4.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nb4.b3.6.weight       | total = 18432.0 | params = 18432.0 | sparsity =   0.00 | shape = (64, 64, 3, 3)\nb4.b3.6.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb4.b3.7.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb4.b3.7.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nb4.b4.1.weight       | total = 16384.0 | params = 16384.0 | sparsity =   0.00 | shape = (64, 512, 1, 1)\nb4.b4.1.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb4.b4.2.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nb4.b4.2.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nc4.b1.0.weight       | total = 32768.0 | params = 32768.0 | sparsity =   0.00 | shape = (128, 512, 1, 1)\nc4.b1.0.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nc4.b1.1.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nc4.b1.1.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nc4.b2.0.weight       | total = 32768.0 | params = 32768.0 | sparsity =   0.00 | shape = (128, 512, 1, 1)\nc4.b2.0.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nc4.b2.1.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nc4.b2.1.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nc4.b2.3.weight       | total = 147456.0 | params = 147456.0 | sparsity =   0.00 | shape = (256, 128, 3, 3)\nc4.b2.3.bias         | total =   128.0 | params =   128.0 | sparsity =   0.00 | shape = (256,)\nc4.b2.4.weight       | total =   128.0 | params =   128.0 | sparsity =   0.00 | shape = (256,)\nc4.b2.4.bias         | total =   128.0 | params =     0.0 | sparsity =   1.00 | shape = (256,)\nc4.b3.0.weight       | total =  6144.0 | params =  6144.0 | sparsity =   0.00 | shape = (24, 512, 1, 1)\nc4.b3.0.bias         | total =    12.0 | params =    12.0 | sparsity =   0.00 | shape = (24,)\nc4.b3.1.weight       | total =    12.0 | params =    12.0 | sparsity =   0.00 | shape = (24,)\nc4.b3.1.bias         | total =    12.0 | params =     0.0 | sparsity =   1.00 | shape = (24,)\nc4.b3.3.weight       | total =  6912.0 | params =  6912.0 | sparsity =   0.00 | shape = (64, 24, 3, 3)\nc4.b3.3.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nc4.b3.4.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nc4.b3.4.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nc4.b3.6.weight       | total = 18432.0 | params = 18432.0 | sparsity =   0.00 | shape = (64, 64, 3, 3)\nc4.b3.6.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nc4.b3.7.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nc4.b3.7.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nc4.b4.1.weight       | total = 16384.0 | params = 16384.0 | sparsity =   0.00 | shape = (64, 512, 1, 1)\nc4.b4.1.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nc4.b4.2.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nc4.b4.2.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nd4.b1.0.weight       | total = 28672.0 | params = 28672.0 | sparsity =   0.00 | shape = (112, 512, 1, 1)\nd4.b1.0.bias         | total =    56.0 | params =    56.0 | sparsity =   0.00 | shape = (112,)\nd4.b1.1.weight       | total =    56.0 | params =    56.0 | sparsity =   0.00 | shape = (112,)\nd4.b1.1.bias         | total =    56.0 | params =     0.0 | sparsity =   1.00 | shape = (112,)\nd4.b2.0.weight       | total = 36864.0 | params = 36864.0 | sparsity =   0.00 | shape = (144, 512, 1, 1)\nd4.b2.0.bias         | total =    72.0 | params =    72.0 | sparsity =   0.00 | shape = (144,)\nd4.b2.1.weight       | total =    72.0 | params =    72.0 | sparsity =   0.00 | shape = (144,)\nd4.b2.1.bias         | total =    72.0 | params =     0.0 | sparsity =   1.00 | shape = (144,)\nd4.b2.3.weight       | total = 186624.0 | params = 186624.0 | sparsity =   0.00 | shape = (288, 144, 3, 3)\nd4.b2.3.bias         | total =   144.0 | params =   144.0 | sparsity =   0.00 | shape = (288,)\nd4.b2.4.weight       | total =   144.0 | params =   144.0 | sparsity =   0.00 | shape = (288,)\nd4.b2.4.bias         | total =   144.0 | params =     0.0 | sparsity =   1.00 | shape = (288,)\nd4.b3.0.weight       | total =  8192.0 | params =  8192.0 | sparsity =   0.00 | shape = (32, 512, 1, 1)\nd4.b3.0.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\nd4.b3.1.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\nd4.b3.1.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\nd4.b3.3.weight       | total =  9216.0 | params =  9216.0 | sparsity =   0.00 | shape = (64, 32, 3, 3)\nd4.b3.3.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nd4.b3.4.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nd4.b3.4.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nd4.b3.6.weight       | total = 18432.0 | params = 18432.0 | sparsity =   0.00 | shape = (64, 64, 3, 3)\nd4.b3.6.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nd4.b3.7.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nd4.b3.7.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\nd4.b4.1.weight       | total = 16384.0 | params = 16384.0 | sparsity =   0.00 | shape = (64, 512, 1, 1)\nd4.b4.1.bias         | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nd4.b4.2.weight       | total =    32.0 | params =    32.0 | sparsity =   0.00 | shape = (64,)\nd4.b4.2.bias         | total =    32.0 | params =     0.0 | sparsity =   1.00 | shape = (64,)\ne4.b1.0.weight       | total = 67584.0 | params = 67584.0 | sparsity =   0.00 | shape = (256, 528, 1, 1)\ne4.b1.0.bias         | total =   128.0 | params =   128.0 | sparsity =   0.00 | shape = (256,)\ne4.b1.1.weight       | total =   128.0 | params =   128.0 | sparsity =   0.00 | shape = (256,)\ne4.b1.1.bias         | total =   128.0 | params =     0.0 | sparsity =   1.00 | shape = (256,)\ne4.b2.0.weight       | total = 42240.0 | params = 42240.0 | sparsity =   0.00 | shape = (160, 528, 1, 1)\ne4.b2.0.bias         | total =    80.0 | params =    80.0 | sparsity =   0.00 | shape = (160,)\ne4.b2.1.weight       | total =    80.0 | params =    80.0 | sparsity =   0.00 | shape = (160,)\ne4.b2.1.bias         | total =    80.0 | params =     0.0 | sparsity =   1.00 | shape = (160,)\ne4.b2.3.weight       | total = 230400.0 | params = 230400.0 | sparsity =   0.00 | shape = (320, 160, 3, 3)\ne4.b2.3.bias         | total =   160.0 | params =   160.0 | sparsity =   0.00 | shape = (320,)\ne4.b2.4.weight       | total =   160.0 | params =   160.0 | sparsity =   0.00 | shape = (320,)\ne4.b2.4.bias         | total =   160.0 | params =     0.0 | sparsity =   1.00 | shape = (320,)\ne4.b3.0.weight       | total =  8448.0 | params =  8448.0 | sparsity =   0.00 | shape = (32, 528, 1, 1)\ne4.b3.0.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\ne4.b3.1.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\ne4.b3.1.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\ne4.b3.3.weight       | total = 18432.0 | params = 18432.0 | sparsity =   0.00 | shape = (128, 32, 3, 3)\ne4.b3.3.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\ne4.b3.4.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\ne4.b3.4.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\ne4.b3.6.weight       | total = 73728.0 | params = 73728.0 | sparsity =   0.00 | shape = (128, 128, 3, 3)\ne4.b3.6.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\ne4.b3.7.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\ne4.b3.7.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\ne4.b4.1.weight       | total = 33792.0 | params = 33792.0 | sparsity =   0.00 | shape = (128, 528, 1, 1)\ne4.b4.1.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\ne4.b4.2.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\ne4.b4.2.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\na5.b1.0.weight       | total = 106496.0 | params = 106496.0 | sparsity =   0.00 | shape = (256, 832, 1, 1)\na5.b1.0.bias         | total =   128.0 | params =   128.0 | sparsity =   0.00 | shape = (256,)\na5.b1.1.weight       | total =   128.0 | params =   128.0 | sparsity =   0.00 | shape = (256,)\na5.b1.1.bias         | total =   128.0 | params =     0.0 | sparsity =   1.00 | shape = (256,)\na5.b2.0.weight       | total = 66560.0 | params = 66560.0 | sparsity =   0.00 | shape = (160, 832, 1, 1)\na5.b2.0.bias         | total =    80.0 | params =    80.0 | sparsity =   0.00 | shape = (160,)\na5.b2.1.weight       | total =    80.0 | params =    80.0 | sparsity =   0.00 | shape = (160,)\na5.b2.1.bias         | total =    80.0 | params =     0.0 | sparsity =   1.00 | shape = (160,)\na5.b2.3.weight       | total = 230400.0 | params = 230400.0 | sparsity =   0.00 | shape = (320, 160, 3, 3)\na5.b2.3.bias         | total =   160.0 | params =   160.0 | sparsity =   0.00 | shape = (320,)\na5.b2.4.weight       | total =   160.0 | params =   160.0 | sparsity =   0.00 | shape = (320,)\na5.b2.4.bias         | total =   160.0 | params =     0.0 | sparsity =   1.00 | shape = (320,)\na5.b3.0.weight       | total = 13312.0 | params = 13312.0 | sparsity =   0.00 | shape = (32, 832, 1, 1)\na5.b3.0.bias         | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na5.b3.1.weight       | total =    16.0 | params =    16.0 | sparsity =   0.00 | shape = (32,)\na5.b3.1.bias         | total =    16.0 | params =     0.0 | sparsity =   1.00 | shape = (32,)\na5.b3.3.weight       | total = 18432.0 | params = 18432.0 | sparsity =   0.00 | shape = (128, 32, 3, 3)\na5.b3.3.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na5.b3.4.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na5.b3.4.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\na5.b3.6.weight       | total = 73728.0 | params = 73728.0 | sparsity =   0.00 | shape = (128, 128, 3, 3)\na5.b3.6.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na5.b3.7.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na5.b3.7.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\na5.b4.1.weight       | total = 53248.0 | params = 53248.0 | sparsity =   0.00 | shape = (128, 832, 1, 1)\na5.b4.1.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na5.b4.2.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\na5.b4.2.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nb5.b1.0.weight       | total = 159744.0 | params = 159744.0 | sparsity =   0.00 | shape = (384, 832, 1, 1)\nb5.b1.0.bias         | total =   192.0 | params =   192.0 | sparsity =   0.00 | shape = (384,)\nb5.b1.1.weight       | total =   192.0 | params =   192.0 | sparsity =   0.00 | shape = (384,)\nb5.b1.1.bias         | total =   192.0 | params =     0.0 | sparsity =   1.00 | shape = (384,)\nb5.b2.0.weight       | total = 79872.0 | params = 79872.0 | sparsity =   0.00 | shape = (192, 832, 1, 1)\nb5.b2.0.bias         | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\nb5.b2.1.weight       | total =    96.0 | params =    96.0 | sparsity =   0.00 | shape = (192,)\nb5.b2.1.bias         | total =    96.0 | params =     0.0 | sparsity =   1.00 | shape = (192,)\nb5.b2.3.weight       | total = 331776.0 | params = 331776.0 | sparsity =   0.00 | shape = (384, 192, 3, 3)\nb5.b2.3.bias         | total =   192.0 | params =   192.0 | sparsity =   0.00 | shape = (384,)\nb5.b2.4.weight       | total =   192.0 | params =   192.0 | sparsity =   0.00 | shape = (384,)\nb5.b2.4.bias         | total =   192.0 | params =     0.0 | sparsity =   1.00 | shape = (384,)\nb5.b3.0.weight       | total = 19968.0 | params = 19968.0 | sparsity =   0.00 | shape = (48, 832, 1, 1)\nb5.b3.0.bias         | total =    24.0 | params =    24.0 | sparsity =   0.00 | shape = (48,)\nb5.b3.1.weight       | total =    24.0 | params =    24.0 | sparsity =   0.00 | shape = (48,)\nb5.b3.1.bias         | total =    24.0 | params =     0.0 | sparsity =   1.00 | shape = (48,)\nb5.b3.3.weight       | total = 27648.0 | params = 27648.0 | sparsity =   0.00 | shape = (128, 48, 3, 3)\nb5.b3.3.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb5.b3.4.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb5.b3.4.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nb5.b3.6.weight       | total = 73728.0 | params = 73728.0 | sparsity =   0.00 | shape = (128, 128, 3, 3)\nb5.b3.6.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb5.b3.7.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb5.b3.7.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nb5.b4.1.weight       | total = 53248.0 | params = 53248.0 | sparsity =   0.00 | shape = (128, 832, 1, 1)\nb5.b4.1.bias         | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb5.b4.2.weight       | total =    64.0 | params =    64.0 | sparsity =   0.00 | shape = (128,)\nb5.b4.2.bias         | total =    64.0 | params =     0.0 | sparsity =   1.00 | shape = (128,)\nlinear.weight        | total = 51200.0 | params = 51200.0 | sparsity =   0.00 | shape = (100, 1024)\nlinear.bias          | total =    50.0 | params =    50.0 | sparsity =   0.00 | shape = (100,)\nTotal: 6258500 Alive: 6250596\n","name":"stdout"},{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"6258500"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The baseline unpruned model has 6258500 params."},{"metadata":{},"cell_type":"markdown","source":"<h2>Scoring</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sparse_size(tensor_shape, param_bits=32, sparsity=0):\n    n_elements = np.prod(tensor_shape)\n    c_size = n_elements * param_bits * (1 - sparsity)\n    if sparsity > 0:\n        c_size += n_elements  # 1 bit binary mask.\n    return c_size\n\n\ndef get_conv_output_size(image_size, filter_size, stride, padding):\n    out_size = np.ceil((image_size - filter_size + 1. + 2 * padding) / stride)\n    return int(out_size)","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_ops(op, input_shape, param_bits=32, sparsity=0):\n    c_in,resolution = input_shape\n    flop_mults = flop_adds = param_count = 0\n    \n    if isinstance(op, MaskedConv2d):\n        \n        resolution = op.input_shape\n        k_size = op.kernel_size\n        c_in = op.in_channels\n        c_out = op.out_channels\n        x =  get_sparse_size([k_size, k_size, c_in, c_out], param_bits, sparsity)\n        #print(x)\n        param_count+=x\n        stride = 1\n        vector_length = (k_size * k_size * c_in) * (1 - sparsity)\n        out_size = get_conv_output_size(resolution, k_size, stride, op.padding)\n        n_output_elements =  out_size**2 * c_out\n        \n        # Each output is the product of a one dot product. Dot product of two\n        # vectors of size n needs n multiplications and n - 1 additions.\n        flop_mults += vector_length * n_output_elements\n        flop_adds += (vector_length - 1) * n_output_elements\n\n        if type(op.bias)!=type(None):\n          # For each output channel we need a bias term.\n          param_count += c_out * param_bits\n          # If we have bias we need one more addition per dot product.\n          flop_adds += n_output_elements\n            \n        out_shape = (c_out,out_size)\n    \n    elif isinstance(op,nn.ReLU):\n        n_muls = 1\n        n_adds = 0\n        flop_mults += n_muls*(c_in*resolution*resolution)\n        flop_adds += n_adds*(c_in*resolution*resolution)\n        out_shape = input_shape\n \n    elif isinstance(op, nn.AdaptiveAvgPool2d):\n        # For each output channel we will make a division.\n        flop_mults += c_in\n        # We have to add values over spatial dimensions.\n        flop_adds += (resolution * resolution - 1) * c_in\n        out_size=op.output_size[0]\n        out_shape = (c_in,out_size)\n    \n    elif isinstance(op, nn.MaxPool2d):\n        # For each output channel we will make max.\n        flop_mults += c_in\n        # We have to add values over spatial dimensions.\n        flop_adds += (resolution * resolution - 1) * c_in\n        out_size=get_conv_output_size(resolution, op.kernel_size, op.stride, op.padding)\n        out_shape = (c_in,out_size)\n        \n    elif isinstance(op, MaskedLinear):\n        c_in, c_out = op.in_features,op.out_features\n        param_count += get_sparse_size([c_in, c_out], param_bits, sparsity)\n        \n        n_elements = c_in * (1 - sparsity)\n        flop_mults += n_elements * c_out\n\n        flop_adds += (n_elements - 1) * c_out\n        out_shape=input_shape\n        if type(op.bias)!=type(None):\n            param_count += c_out * param_bits\n            flop_adds += c_out\n            \n    elif isinstance(op,nn.BatchNorm2d):     #Batch Norm does not require any flops as it is merged into conv\n        c_in = c_out = op.num_features\n        param_count += get_sparse_size([c_in], param_bits, sparsity)\n        n_elements = c_in * (1 - sparsity)\n        flop_mults = 0\n        flop_adds = 0\n        out_size = input_shape\n        if type(op.bias)!=type(None):\n            param_count += c_out * param_bits\n            flop_adds += c_out\n        out_shape = input_shape\n        \n    elif isinstance(op,nn.Dropout2d):  #Nothing for Dropout\n        return 0,0,0,(c_in,resolution)\n    \n    else:\n        raise ValueError('Encountered unknown operation %s.' % str(op))\n    return param_count, flop_mults, flop_adds,out_shape\n\n","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_total_paramflops(param_bits):\n    totalparams=0\n    totalbits=0\n    totalmults=0\n    totaladds=0\n    input_shape = (3,32)\n    for name, op in ops:\n        sp = 0\n        if isinstance(op,MaskedLinear) or isinstance(op,MaskedConv2d):\n            wname = name+'.weight'\n            weight_tensor = model_dict[wname]\n            actual_size = np.prod(weight_tensor.shape)\n            non_zeros = np.count_nonzero(weight_tensor.numpy())\n            sp = 1 - non_zeros/actual_size\n        \n        bit_count,mults,adds,out_shape = count_ops(op,input_shape,param_bits = param_bits, sparsity = sp)\n        print(name,' | bits:',bit_count,' | params:',bit_count/32,'| sparsity:',sp,' | mults:',mults,' | adds',adds)\n        totalbits +=bit_count\n        totalparams +=bit_count/32\n        totalmults+= mults\n        totaladds+=adds\n        input_shape = out_shape\n    print('_'*80,'\\n')\n    print('Total bits:',totalbits,'| Total params:',totalparams,'| Total Mult-Adds:',(totalmults+totaladds))\n    print('Total (Mbytes):',totalbits/8/1e6,'| Total params(M):',totalparams/1e6,'| Total Mult-Adds(B):',(totalmults+totaladds)/1e9)\n    return totalbits,totalparams,totalmults,totaladds","execution_count":62,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing the scorer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.randn(1,3,32,32)\ntest = net(a)\nops = [(name,module) for name,module in net.named_modules() if type(module) not in [GoogleNet, Inception, nn.Sequential]]","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dict = net.state_dict()\ncalculate_total_paramflops(32)","execution_count":64,"outputs":[{"output_type":"stream","text":"prelayer.0  | bits: 172032.0  | params: 5376.0 | sparsity: 0.0  | mults: 5308416.0  | adds 5308416.0\nprelayer.1  | bits: 12288  | params: 384.0 | sparsity: 0  | mults: 0  | adds 192\nprelayer.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 196608  | adds 0\na3.b1.0  | bits: 395264.0  | params: 12352.0 | sparsity: 0.0  | mults: 12582912.0  | adds 12582912.0\na3.b1.1  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\na3.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\na3.b2.0  | bits: 592896.0  | params: 18528.0 | sparsity: 0.0  | mults: 18874368.0  | adds 18874368.0\na3.b2.1  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 96\na3.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 98304  | adds 0\na3.b2.3  | bits: 3543040.0  | params: 110720.0 | sparsity: 0.0  | mults: 113246208.0  | adds 113246208.0\na3.b2.4  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\na3.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 131072  | adds 0\na3.b3.0  | bits: 98816.0  | params: 3088.0 | sparsity: 0.0  | mults: 3145728.0  | adds 3145728.0\na3.b3.1  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 16\na3.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\na3.b3.3  | bits: 148480.0  | params: 4640.0 | sparsity: 0.0  | mults: 4718592.0  | adds 4718592.0\na3.b3.4  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\na3.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\na3.b3.6  | bits: 295936.0  | params: 9248.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\na3.b3.7  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\na3.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\na3.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32  | adds 32736\na3.b4.1  | bits: 197632.0  | params: 6176.0 | sparsity: 0.0  | mults: 6291456.0  | adds 6291456.0\na3.b4.2  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\na3.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nb3.b1.0  | bits: 1052672.0  | params: 32896.0 | sparsity: 0.0  | mults: 33554432.0  | adds 33554432.0\nb3.b1.1  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nb3.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 131072  | adds 0\nb3.b2.0  | bits: 1052672.0  | params: 32896.0 | sparsity: 0.0  | mults: 33554432.0  | adds 33554432.0\nb3.b2.1  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nb3.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 131072  | adds 0\nb3.b2.3  | bits: 7084032.0  | params: 221376.0 | sparsity: 0.0  | mults: 226492416.0  | adds 226492416.0\nb3.b2.4  | bits: 12288  | params: 384.0 | sparsity: 0  | mults: 0  | adds 192\nb3.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 196608  | adds 0\nb3.b3.0  | bits: 263168.0  | params: 8224.0 | sparsity: 0.0  | mults: 8388608.0  | adds 8388608.0\nb3.b3.1  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\nb3.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nb3.b3.3  | bits: 887808.0  | params: 27744.0 | sparsity: 0.0  | mults: 28311552.0  | adds 28311552.0\nb3.b3.4  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 96\nb3.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 98304  | adds 0\nb3.b3.6  | bits: 2657280.0  | params: 83040.0 | sparsity: 0.0  | mults: 84934656.0  | adds 84934656.0\nb3.b3.7  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 96\nb3.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 98304  | adds 0\nb3.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 96  | adds 98208\nb3.b4.1  | bits: 526336.0  | params: 16448.0 | sparsity: 0.0  | mults: 16777216.0  | adds 16777216.0\nb3.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nb3.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\nmaxpool  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 65472\na4.b1.0  | bits: 2955264.0  | params: 92352.0 | sparsity: 0.0  | mults: 23592960.0  | adds 23592960.0\na4.b1.1  | bits: 12288  | params: 384.0 | sparsity: 0  | mults: 0  | adds 192\na4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 49152  | adds 0\na4.b2.0  | bits: 1477632.0  | params: 46176.0 | sparsity: 0.0  | mults: 11796480.0  | adds 11796480.0\na4.b2.1  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 96\na4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 24576  | adds 0\na4.b2.3  | bits: 5757440.0  | params: 179920.0 | sparsity: 0.0  | mults: 46006272.0  | adds 46006272.0\na4.b2.4  | bits: 13312  | params: 416.0 | sparsity: 0  | mults: 0  | adds 208\na4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 53248  | adds 0\na4.b3.0  | bits: 246272.0  | params: 7696.0 | sparsity: 0.0  | mults: 1966080.0  | adds 1966080.0\na4.b3.1  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 16\na4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 4096  | adds 0\na4.b3.3  | bits: 222720.0  | params: 6960.0 | sparsity: 0.0  | mults: 1769472.0  | adds 1769472.0\na4.b3.4  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 48\na4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 12288  | adds 0\na4.b3.6  | bits: 665088.0  | params: 20784.0 | sparsity: 0.0  | mults: 5308416.0  | adds 5308416.0\na4.b3.7  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 48\na4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 12288  | adds 0\na4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 48  | adds 12240\na4.b4.1  | bits: 985088.0  | params: 30784.0 | sparsity: 0.0  | mults: 7864320.0  | adds 7864320.0\na4.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\na4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nb4.b1.0  | bits: 2626560.0  | params: 82080.0 | sparsity: 0.0  | mults: 20971520.0  | adds 20971520.0\nb4.b1.1  | bits: 10240  | params: 320.0 | sparsity: 0  | mults: 0  | adds 160\nb4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 40960  | adds 0\nb4.b2.0  | bits: 1838592.0  | params: 57456.0 | sparsity: 0.0  | mults: 14680064.0  | adds 14680064.0\nb4.b2.1  | bits: 7168  | params: 224.0 | sparsity: 0  | mults: 0  | adds 112\nb4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 28672  | adds 0\nb4.b2.3  | bits: 7232512.0  | params: 226016.0 | sparsity: 0.0  | mults: 57802752.0  | adds 57802752.0\nb4.b2.4  | bits: 14336  | params: 448.0 | sparsity: 0  | mults: 0  | adds 224\nb4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 57344  | adds 0\nb4.b3.0  | bits: 393984.0  | params: 12312.0 | sparsity: 0.0  | mults: 3145728.0  | adds 3145728.0\nb4.b3.1  | bits: 1536  | params: 48.0 | sparsity: 0  | mults: 0  | adds 24\nb4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 6144  | adds 0\nb4.b3.3  | bits: 444416.0  | params: 13888.0 | sparsity: 0.0  | mults: 3538944.0  | adds 3538944.0\nb4.b3.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nb4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nb4.b3.6  | bits: 1181696.0  | params: 36928.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\nb4.b3.7  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nb4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nb4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 16320\nb4.b4.1  | bits: 1050624.0  | params: 32832.0 | sparsity: 0.0  | mults: 8388608.0  | adds 8388608.0\nb4.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nb4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nc4.b1.0  | bits: 2101248.0  | params: 65664.0 | sparsity: 0.0  | mults: 16777216.0  | adds 16777216.0\nc4.b1.1  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nc4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nc4.b2.0  | bits: 2101248.0  | params: 65664.0 | sparsity: 0.0  | mults: 16777216.0  | adds 16777216.0\nc4.b2.1  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nc4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nc4.b2.3  | bits: 9445376.0  | params: 295168.0 | sparsity: 0.0  | mults: 75497472.0  | adds 75497472.0\nc4.b2.4  | bits: 16384  | params: 512.0 | sparsity: 0  | mults: 0  | adds 256\nc4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\nc4.b3.0  | bits: 393984.0  | params: 12312.0 | sparsity: 0.0  | mults: 3145728.0  | adds 3145728.0\nc4.b3.1  | bits: 1536  | params: 48.0 | sparsity: 0  | mults: 0  | adds 24\nc4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 6144  | adds 0\nc4.b3.3  | bits: 444416.0  | params: 13888.0 | sparsity: 0.0  | mults: 3538944.0  | adds 3538944.0\nc4.b3.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nc4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nc4.b3.6  | bits: 1181696.0  | params: 36928.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\nc4.b3.7  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nc4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nc4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 16320\nc4.b4.1  | bits: 1050624.0  | params: 32832.0 | sparsity: 0.0  | mults: 8388608.0  | adds 8388608.0\nc4.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nc4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nd4.b1.0  | bits: 1838592.0  | params: 57456.0 | sparsity: 0.0  | mults: 14680064.0  | adds 14680064.0\nd4.b1.1  | bits: 7168  | params: 224.0 | sparsity: 0  | mults: 0  | adds 112\nd4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 28672  | adds 0\nd4.b2.0  | bits: 2363904.0  | params: 73872.0 | sparsity: 0.0  | mults: 18874368.0  | adds 18874368.0\nd4.b2.1  | bits: 9216  | params: 288.0 | sparsity: 0  | mults: 0  | adds 144\nd4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 36864  | adds 0\nd4.b2.3  | bits: 11953152.0  | params: 373536.0 | sparsity: 0.0  | mults: 95551488.0  | adds 95551488.0\nd4.b2.4  | bits: 18432  | params: 576.0 | sparsity: 0  | mults: 0  | adds 288\nd4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 73728  | adds 0\nd4.b3.0  | bits: 525312.0  | params: 16416.0 | sparsity: 0.0  | mults: 4194304.0  | adds 4194304.0\nd4.b3.1  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\nd4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nd4.b3.3  | bits: 591872.0  | params: 18496.0 | sparsity: 0.0  | mults: 4718592.0  | adds 4718592.0\nd4.b3.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nd4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nd4.b3.6  | bits: 1181696.0  | params: 36928.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\nd4.b3.7  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nd4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nd4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 16320\nd4.b4.1  | bits: 1050624.0  | params: 32832.0 | sparsity: 0.0  | mults: 8388608.0  | adds 8388608.0\nd4.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 64\nd4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\ne4.b1.0  | bits: 4333568.0  | params: 135424.0 | sparsity: 0.0  | mults: 34603008.0  | adds 34603008.0\ne4.b1.1  | bits: 16384  | params: 512.0 | sparsity: 0  | mults: 0  | adds 256\ne4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\ne4.b2.0  | bits: 2708480.0  | params: 84640.0 | sparsity: 0.0  | mults: 21626880.0  | adds 21626880.0\ne4.b2.1  | bits: 10240  | params: 320.0 | sparsity: 0  | mults: 0  | adds 160\ne4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 40960  | adds 0\ne4.b2.3  | bits: 14755840.0  | params: 461120.0 | sparsity: 0.0  | mults: 117964800.0  | adds 117964800.0\ne4.b2.4  | bits: 20480  | params: 640.0 | sparsity: 0  | mults: 0  | adds 320\ne4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 81920  | adds 0\ne4.b3.0  | bits: 541696.0  | params: 16928.0 | sparsity: 0.0  | mults: 4325376.0  | adds 4325376.0\ne4.b3.1  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\ne4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\ne4.b3.3  | bits: 1183744.0  | params: 36992.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\ne4.b3.4  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\ne4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\ne4.b3.6  | bits: 4722688.0  | params: 147584.0 | sparsity: 0.0  | mults: 37748736.0  | adds 37748736.0\ne4.b3.7  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\ne4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\ne4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 32640\ne4.b4.1  | bits: 2166784.0  | params: 67712.0 | sparsity: 0.0  | mults: 17301504.0  | adds 17301504.0\ne4.b4.2  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\ne4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\na5.b1.0  | bits: 6823936.0  | params: 213248.0 | sparsity: 0.0  | mults: 13631488.0  | adds 13631488.0\na5.b1.1  | bits: 16384  | params: 512.0 | sparsity: 0  | mults: 0  | adds 256\na5.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\na5.b2.0  | bits: 4264960.0  | params: 133280.0 | sparsity: 0.0  | mults: 8519680.0  | adds 8519680.0\na5.b2.1  | bits: 10240  | params: 320.0 | sparsity: 0  | mults: 0  | adds 160\na5.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 10240  | adds 0\n","name":"stdout"},{"output_type":"stream","text":"a5.b2.3  | bits: 14755840.0  | params: 461120.0 | sparsity: 0.0  | mults: 29491200.0  | adds 29491200.0\na5.b2.4  | bits: 20480  | params: 640.0 | sparsity: 0  | mults: 0  | adds 320\na5.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 20480  | adds 0\na5.b3.0  | bits: 852992.0  | params: 26656.0 | sparsity: 0.0  | mults: 1703936.0  | adds 1703936.0\na5.b3.1  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 32\na5.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 2048  | adds 0\na5.b3.3  | bits: 1183744.0  | params: 36992.0 | sparsity: 0.0  | mults: 2359296.0  | adds 2359296.0\na5.b3.4  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\na5.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\na5.b3.6  | bits: 4722688.0  | params: 147584.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\na5.b3.7  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\na5.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\na5.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 8064\na5.b4.1  | bits: 3411968.0  | params: 106624.0 | sparsity: 0.0  | mults: 6815744.0  | adds 6815744.0\na5.b4.2  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\na5.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nb5.b1.0  | bits: 10235904.0  | params: 319872.0 | sparsity: 0.0  | mults: 20447232.0  | adds 20447232.0\nb5.b1.1  | bits: 24576  | params: 768.0 | sparsity: 0  | mults: 0  | adds 384\nb5.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 24576  | adds 0\nb5.b2.0  | bits: 5117952.0  | params: 159936.0 | sparsity: 0.0  | mults: 10223616.0  | adds 10223616.0\nb5.b2.1  | bits: 12288  | params: 384.0 | sparsity: 0  | mults: 0  | adds 192\nb5.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 12288  | adds 0\nb5.b2.3  | bits: 21245952.0  | params: 663936.0 | sparsity: 0.0  | mults: 42467328.0  | adds 42467328.0\nb5.b2.4  | bits: 24576  | params: 768.0 | sparsity: 0  | mults: 0  | adds 384\nb5.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 24576  | adds 0\nb5.b3.0  | bits: 1279488.0  | params: 39984.0 | sparsity: 0.0  | mults: 2555904.0  | adds 2555904.0\nb5.b3.1  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 48\nb5.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 3072  | adds 0\nb5.b3.3  | bits: 1773568.0  | params: 55424.0 | sparsity: 0.0  | mults: 3538944.0  | adds 3538944.0\nb5.b3.4  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nb5.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nb5.b3.6  | bits: 4722688.0  | params: 147584.0 | sparsity: 0.0  | mults: 9437184.0  | adds 9437184.0\nb5.b3.7  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nb5.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nb5.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 8064\nb5.b4.1  | bits: 3411968.0  | params: 106624.0 | sparsity: 0.0  | mults: 6815744.0  | adds 6815744.0\nb5.b4.2  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 128\nb5.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\navgpool  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 8064\ndropout  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 0  | adds 0\nlinear  | bits: 3280000.0  | params: 102500.0 | sparsity: 0.0  | mults: 102400.0  | adds 102400.0\n________________________________________________________________________________ \n\nTotal bits: 200272000.0 | Total params: 6258500.0 | Total Mult-Adds: 3046574816.0\nTotal (Mbytes): 25.034 | Total params(M): 6.2585 | Total Mult-Adds(B): 3.046574816\n","name":"stdout"},{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"(200272000.0, 6258500.0, 1524404144.0, 1522170672.0)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<p>Baseline model has 6.26M params and 3.04B mult-adds.</p>\n"},{"metadata":{},"cell_type":"markdown","source":"<h3> Calculating Final Score:</h3>\n<p>Scoring the final checkpoint (With Freebie Quantization):</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_checkpoint = torch.load(final_path)\nnet.load_state_dict(score_checkpoint['model_state_dict'])\nmodel_dict = net.state_dict()\ntotalbits,totalparams,totalmults,totaladds = calculate_total_paramflops(16)","execution_count":65,"outputs":[{"output_type":"stream","text":"prelayer.0  | bits: 60480.0  | params: 1890.0 | sparsity: 0.37037037037037035  | mults: 3342336.0  | adds 3342336.0\nprelayer.1  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 192\nprelayer.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 196608  | adds 0\na3.b1.0  | bits: 120688.0  | params: 3771.5 | sparsity: 0.453857421875  | mults: 6872064.0  | adds 6872064.0\na3.b1.1  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\na3.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\na3.b2.0  | bits: 196880.0  | params: 6152.5 | sparsity: 0.40011935763888884  | mults: 11322368.0  | adds 11322368.0\na3.b2.1  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 96\na3.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 98304  | adds 0\na3.b2.3  | bits: 1081488.0  | params: 33796.5 | sparsity: 0.4524649160879629  | mults: 62006272.00000001  | adds 62006272.00000001\na3.b2.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\na3.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 131072  | adds 0\na3.b3.0  | bits: 31040.0  | params: 970.0 | sparsity: 0.43619791666666663  | mults: 1773568.0  | adds 1773568.0\na3.b3.1  | bits: 512  | params: 16.0 | sparsity: 0  | mults: 0  | adds 16\na3.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\na3.b3.3  | bits: 50432.0  | params: 1576.0 | sparsity: 0.38541666666666663  | mults: 2899968.0  | adds 2899968.0\na3.b3.4  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\na3.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\na3.b3.6  | bits: 102656.0  | params: 3208.0 | sparsity: 0.36979166666666663  | mults: 5947392.0  | adds 5947392.0\na3.b3.7  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\na3.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\na3.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32  | adds 32736\na3.b4.1  | bits: 53408.0  | params: 1669.0 | sparsity: 0.5244140625  | mults: 2992128.0  | adds 2992128.0\na3.b4.2  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\na3.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nb3.b1.0  | bits: 325904.0  | params: 10184.5 | sparsity: 0.444793701171875  | mults: 18629632.0  | adds 18629632.0\nb3.b1.1  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nb3.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 131072  | adds 0\nb3.b2.0  | bits: 326048.0  | params: 10189.0 | sparsity: 0.44451904296875  | mults: 18638848.0  | adds 18638848.0\nb3.b2.1  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nb3.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 131072  | adds 0\nb3.b2.3  | bits: 2189616.0  | params: 68425.5 | sparsity: 0.4446478949652778  | mults: 125783040.0  | adds 125783040.0\nb3.b2.4  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 192\nb3.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 196608  | adds 0\nb3.b3.0  | bits: 79088.0  | params: 2471.5 | sparsity: 0.4630126953125  | mults: 4504576.0  | adds 4504576.0\nb3.b3.1  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\nb3.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nb3.b3.3  | bits: 290320.0  | params: 9072.5 | sparsity: 0.4096860532407407  | mults: 16712704.000000002  | adds 16712704.000000002\nb3.b3.4  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 96\nb3.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 98304  | adds 0\nb3.b3.6  | bits: 855808.0  | params: 26744.0 | sparsity: 0.41878858024691357  | mults: 49364992.0  | adds 49364992.0\nb3.b3.7  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 96\nb3.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 98304  | adds 0\nb3.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 96  | adds 98208\nb3.b4.1  | bits: 153408.0  | params: 4794.0 | sparsity: 0.481201171875  | mults: 8704000.0  | adds 8704000.0\nb3.b4.2  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nb3.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\nmaxpool  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 65472\na4.b1.0  | bits: 889536.0  | params: 27798.0 | sparsity: 0.461328125  | mults: 12708864.0  | adds 12708864.0\na4.b1.1  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 192\na4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 49152  | adds 0\na4.b2.0  | bits: 440256.0  | params: 13758.0 | sparsity: 0.46744791666666663  | mults: 6282240.000000001  | adds 6282240.000000001\na4.b2.1  | bits: 3072  | params: 96.0 | sparsity: 0  | mults: 0  | adds 96\na4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 24576  | adds 0\na4.b2.3  | bits: 1775648.0000000002  | params: 55489.00000000001 | sparsity: 0.4461249109686609  | mults: 25481728.000000004  | adds 25481728.000000004\na4.b2.4  | bits: 6656  | params: 208.0 | sparsity: 0  | mults: 0  | adds 208\na4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 53248  | adds 0\na4.b3.0  | bits: 77904.0  | params: 2434.5 | sparsity: 0.4305989583333333  | mults: 1119488.0  | adds 1119488.0\na4.b3.1  | bits: 512  | params: 16.0 | sparsity: 0  | mults: 0  | adds 16\na4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 4096  | adds 0\na4.b3.3  | bits: 72160.0  | params: 2255.0 | sparsity: 0.4169560185185185  | mults: 1031680.0000000001  | adds 1031680.0000000001\na4.b3.4  | bits: 1536  | params: 48.0 | sparsity: 0  | mults: 0  | adds 48\na4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 12288  | adds 0\na4.b3.6  | bits: 194528.0  | params: 6079.0 | sparsity: 0.478491512345679  | mults: 2768384.0  | adds 2768384.0\na4.b3.7  | bits: 1536  | params: 48.0 | sparsity: 0  | mults: 0  | adds 48\na4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 12288  | adds 0\na4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 48  | adds 12240\na4.b4.1  | bits: 297584.0  | params: 9299.5 | sparsity: 0.45914713541666663  | mults: 4253440.0  | adds 4253440.0\na4.b4.2  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\na4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nb4.b1.0  | bits: 827776.0  | params: 25868.0 | sparsity: 0.43291015624999996  | mults: 11892736.0  | adds 11892736.0\nb4.b1.1  | bits: 5120  | params: 160.0 | sparsity: 0  | mults: 0  | adds 160\nb4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 40960  | adds 0\nb4.b2.0  | bits: 561296.0  | params: 17540.5 | sparsity: 0.4526890345982143  | mults: 8034560.0  | adds 8034560.0\nb4.b2.1  | bits: 3584  | params: 112.0 | sparsity: 0  | mults: 0  | adds 112\nb4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 28672  | adds 0\nb4.b2.3  | bits: 2160784.0  | params: 67524.5 | sparsity: 0.4653796414399093  | mults: 30902528.0  | adds 30902528.0\nb4.b2.4  | bits: 7168  | params: 224.0 | sparsity: 0  | mults: 0  | adds 224\nb4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 57344  | adds 0\nb4.b3.0  | bits: 119008.0  | params: 3719.0 | sparsity: 0.45914713541666663  | mults: 1701376.0  | adds 1701376.0\nb4.b3.1  | bits: 768  | params: 24.0 | sparsity: 0  | mults: 0  | adds 24\nb4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 6144  | adds 0\nb4.b3.3  | bits: 147456.0  | params: 4608.0 | sparsity: 0.4004629629629629  | mults: 2121728.0  | adds 2121728.0\nb4.b3.4  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nb4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nb4.b3.6  | bits: 390896.0  | params: 12215.5 | sparsity: 0.4015028211805556  | mults: 5648128.0  | adds 5648128.0\nb4.b3.7  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nb4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nb4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 16320\nb4.b4.1  | bits: 333744.0  | params: 10429.5 | sparsity: 0.427886962890625  | mults: 4799232.0  | adds 4799232.0\nb4.b4.2  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nb4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nc4.b1.0  | bits: 660432.0  | params: 20638.5 | sparsity: 0.4346160888671875  | mults: 9485568.0  | adds 9485568.0\nc4.b1.1  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nc4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nc4.b2.0  | bits: 647072.0  | params: 20221.0 | sparsity: 0.447357177734375  | mults: 9271808.0  | adds 9271808.0\nc4.b2.1  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nc4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\nc4.b2.3  | bits: 2776192.0  | params: 86756.0 | sparsity: 0.47501627604166663  | mults: 39634944.0  | adds 39634944.0\nc4.b2.4  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 256\nc4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\nc4.b3.0  | bits: 117040.0  | params: 3657.5 | sparsity: 0.46915690104166663  | mults: 1669888.0  | adds 1669888.0\nc4.b3.1  | bits: 768  | params: 24.0 | sparsity: 0  | mults: 0  | adds 24\nc4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 6144  | adds 0\nc4.b3.3  | bits: 111680.0  | params: 3490.0 | sparsity: 0.5622106481481481  | mults: 1549312.0  | adds 1549312.0\nc4.b3.4  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nc4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nc4.b3.6  | bits: 250384.00000000006  | params: 7824.500000000002 | sparsity: 0.6397298177083333  | mults: 3399936.000000001  | adds 3399936.000000001\nc4.b3.7  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nc4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nc4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 16320\nc4.b4.1  | bits: 336464.0  | params: 10514.5 | sparsity: 0.422698974609375  | mults: 4842752.0  | adds 4842752.0\nc4.b4.2  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nc4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nd4.b1.0  | bits: 539744.0  | params: 16867.0 | sparsity: 0.4761788504464286  | mults: 7689728.0  | adds 7689728.0\nd4.b1.1  | bits: 3584  | params: 112.0 | sparsity: 0  | mults: 0  | adds 112\nd4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 28672  | adds 0\nd4.b2.0  | bits: 701664.0  | params: 21927.0 | sparsity: 0.46964518229166663  | mults: 10010112.0  | adds 10010112.0\nd4.b2.1  | bits: 4608  | params: 144.0 | sparsity: 0  | mults: 0  | adds 144\nd4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 36864  | adds 0\nd4.b2.3  | bits: 3627584.0  | params: 113362.0 | sparsity: 0.4558363340192044  | mults: 51995648.0  | adds 51995648.0\nd4.b2.4  | bits: 9216  | params: 288.0 | sparsity: 0  | mults: 0  | adds 288\nd4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 73728  | adds 0\nd4.b3.0  | bits: 154800.0  | params: 4837.5 | sparsity: 0.47393798828125  | mults: 2206464.0  | adds 2206464.0\nd4.b3.1  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\nd4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nd4.b3.3  | bits: 181952.0  | params: 5686.0 | sparsity: 0.44900173611111116  | mults: 2599936.0  | adds 2599936.0\nd4.b3.4  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nd4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nd4.b3.6  | bits: 331648.0  | params: 10364.0 | sparsity: 0.501953125  | mults: 4700160.0  | adds 4700160.0\nd4.b3.7  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nd4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\nd4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 64  | adds 16320\nd4.b4.1  | bits: 326224.0  | params: 10194.5 | sparsity: 0.442230224609375  | mults: 4678912.0  | adds 4678912.0\nd4.b4.2  | bits: 2048  | params: 64.0 | sparsity: 0  | mults: 0  | adds 64\nd4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\ne4.b1.0  | bits: 1333696.0  | params: 41678.0 | sparsity: 0.4477095170454546  | mults: 19110912.0  | adds 19110912.0\ne4.b1.1  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 256\ne4.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 65536  | adds 0\ne4.b2.0  | bits: 838320.0  | params: 26197.5 | sparsity: 0.4441879734848485  | mults: 12020480.0  | adds 12020480.0\ne4.b2.1  | bits: 5120  | params: 160.0 | sparsity: 0  | mults: 0  | adds 160\ne4.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 40960  | adds 0\ne4.b2.3  | bits: 4615536.0  | params: 144235.5 | sparsity: 0.4371723090277778  | mults: 66393856.0  | adds 66393856.0\ne4.b2.4  | bits: 10240  | params: 320.0 | sparsity: 0  | mults: 0  | adds 320\ne4.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 81920  | adds 0\ne4.b3.0  | bits: 159936.0  | params: 4998.0 | sparsity: 0.4727746212121212  | mults: 2280448.0  | adds 2280448.0\ne4.b3.1  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\ne4.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\ne4.b3.3  | bits: 129536.00000000003  | params: 4048.000000000001 | sparsity: 0.8463541666666666  | mults: 1449984.0000000005  | adds 1449984.0000000005\ne4.b3.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\ne4.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\ne4.b3.6  | bits: 269983.9999999999  | params: 8436.999999999996 | sparsity: 0.9489339192708334  | mults: 1927679.9999999986  | adds 1927679.9999999986\ne4.b3.7  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\n","name":"stdout"},{"output_type":"stream","text":"e4.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\ne4.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 32640\ne4.b4.1  | bits: 673376.0  | params: 21043.0 | sparsity: 0.4416725852272727  | mults: 9659904.0  | adds 9659904.0\ne4.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\ne4.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 32768  | adds 0\na5.b1.0  | bits: 1705632.0  | params: 53301.0 | sparsity: 0.5632042518028846  | mults: 5954176.0  | adds 5954176.0\na5.b1.1  | bits: 8192  | params: 256.0 | sparsity: 0  | mults: 0  | adds 256\na5.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 16384  | adds 0\na5.b2.0  | bits: 1250032.0  | params: 39063.5 | sparsity: 0.47681039663461533  | mults: 4457408.0  | adds 4457408.0\na5.b2.1  | bits: 5120  | params: 160.0 | sparsity: 0  | mults: 0  | adds 160\na5.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 10240  | adds 0\na5.b2.3  | bits: 4654112.0  | params: 145441.0 | sparsity: 0.4319401041666666  | mults: 16752768.0  | adds 16752768.0\na5.b2.4  | bits: 10240  | params: 320.0 | sparsity: 0  | mults: 0  | adds 320\na5.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 20480  | adds 0\na5.b3.0  | bits: 253376.0  | params: 7918.0 | sparsity: 0.4689002403846154  | mults: 904960.0  | adds 904960.0\na5.b3.1  | bits: 1024  | params: 32.0 | sparsity: 0  | mults: 0  | adds 32\na5.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 2048  | adds 0\na5.b3.3  | bits: 337056.0  | params: 10533.0 | sparsity: 0.4945203993055556  | mults: 1192576.0  | adds 1192576.0\na5.b3.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\na5.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\na5.b3.6  | bits: 1068912.0  | params: 33403.5 | sparsity: 0.6103040907118056  | mults: 3677632.0  | adds 3677632.0\na5.b3.7  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\na5.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\na5.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 8064\na5.b4.1  | bits: 1062912.0  | params: 33216.0 | sparsity: 0.43990384615384615  | mults: 3817472.0  | adds 3817472.0\na5.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\na5.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nb5.b1.0  | bits: 2761120.0  | params: 86285.0 | sparsity: 0.5235564403044872  | mults: 9741952.0  | adds 9741952.0\nb5.b1.1  | bits: 12288  | params: 384.0 | sparsity: 0  | mults: 0  | adds 384\nb5.b1.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 24576  | adds 0\nb5.b2.0  | bits: 1414160.0000000002  | params: 44192.50000000001 | sparsity: 0.510410406650641  | mults: 5005376.000000001  | adds 5005376.000000001\nb5.b2.1  | bits: 6144  | params: 192.0 | sparsity: 0  | mults: 0  | adds 192\nb5.b2.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 12288  | adds 0\nb5.b2.3  | bits: 7227519.999999999  | params: 225859.99999999997 | sparsity: 0.3823181905864198  | mults: 26231295.999999996  | adds 26231295.999999996\nb5.b2.4  | bits: 12288  | params: 384.0 | sparsity: 0  | mults: 0  | adds 384\nb5.b2.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 24576  | adds 0\nb5.b3.0  | bits: 354080.0  | params: 11065.0 | sparsity: 0.5095653044871795  | mults: 1253504.0  | adds 1253504.0\nb5.b3.1  | bits: 1536  | params: 48.0 | sparsity: 0  | mults: 0  | adds 48\nb5.b3.2  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 3072  | adds 0\nb5.b3.3  | bits: 423072.0  | params: 13221.0 | sparsity: 0.5866247106481481  | mults: 1462912.0  | adds 1462912.0\nb5.b3.4  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nb5.b3.5  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nb5.b3.6  | bits: 1077872.0  | params: 33683.5 | sparsity: 0.60650634765625  | mults: 3713472.0  | adds 3713472.0\nb5.b3.7  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nb5.b3.8  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\nb5.b4.0  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 8064\nb5.b4.1  | bits: 925344.0  | params: 28917.0 | sparsity: 0.5206392728365384  | mults: 3267200.0  | adds 3267200.0\nb5.b4.2  | bits: 4096  | params: 128.0 | sparsity: 0  | mults: 0  | adds 128\nb5.b4.3  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 8192  | adds 0\navgpool  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 128  | adds 8064\ndropout  | bits: 0  | params: 0.0 | sparsity: 0  | mults: 0  | adds 0\nlinear  | bits: 1304272.0  | params: 40758.5 | sparsity: 0.26741210937500004  | mults: 75017.0  | adds 75017.0\n________________________________________________________________________________ \n\nTotal bits: 59061504.0 | Total params: 1845672.0 | Total Mult-Adds: 1627530482.0\nTotal (Mbytes): 7.382688 | Total params(M): 1.845672 | Total Mult-Adds(B): 1.627530482\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_score = (totalparams/(36.5*1e6)) + ((totalmults+totaladds)/(10.49*1e9))\nprint('Final Score: ', final_score)","execution_count":66,"outputs":[{"output_type":"stream","text":"Final Score:  0.2057170217506562\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The final score is 0.2057 {1.84M params, 1.62B mult-adds}"},{"metadata":{},"cell_type":"markdown","source":"<h3>Evaluation and Training Utilities</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_training():\n    net.eval()\n\n    test_loss = 0.0 # cost function error\n    correct = 0.0\n    k= 0\n    with torch.no_grad():\n        for images, labels in testloader:\n            images = Variable(images)\n            labels = Variable(labels)\n            images = images.cuda()\n            labels = labels.cuda()\n            temp = net(images)\n            loss = loss_function(temp, labels)\n            test_loss += loss.item()\n            _, preds = temp.max(1)\n            del temp\n            torch.cuda.empty_cache()\n            x= preds.eq(labels).sum()\n            print(x)\n            correct = correct+x\n        k=k+1\n        if k%10 == 0:\n            print(k)\n\n    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n        test_loss / len(trainloader.dataset),\n        correct.float() / len(testloader.dataset)\n    ))\n    return correct.float() / len(testloader.dataset)","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(initial_epoch,final_epoch):\n    net.train()\n    for epoch in range(initial_epoch,final_epoch):  \n        if epoch > w:\n            train_scheduler.step(epoch)\n        for batch_index, (data, target) in enumerate(trainloader):\n            if epoch <= w:\n                warmup_scheduler.step()\n                print('warmed up')\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            output = net(data)\n            loss = loss_function(output, target)\n            loss.backward()\n\n            # zero-out all the gradients corresponding to the pruned connections\n            for name, p in net.named_parameters():\n                if 'mask' in name:\n                    continue\n                tensor = p.data.cpu().numpy()\n                grad_tensor = p.grad.data.cpu().numpy()\n                grad_tensor = np.where(tensor==0, 0, grad_tensor)\n                p.grad.data = torch.from_numpy(grad_tensor).cuda()\n\n            optimizer.step()\n            \n            print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n            loss.item(),\n            optimizer.param_groups[0]['lr'],\n            epoch=epoch,\n            trained_samples=batch_index * batchsize + len(data),\n            total_samples=len(trainloader.dataset)\n            ))\n","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WarmUpLR(_LRScheduler):\n    \"\"\"warmup_training learning rate scheduler\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        total_iters: totoal_iters of warmup phase\n    \"\"\"\n    def __init__(self, optimizer, total_iters, last_epoch=-1):\n        \n        self.total_iters = total_iters\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"we will use the first m batches, and set the learning\n        rate to base_lr * m / total_iters\n        \"\"\"\n        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LabelSmoothedCrossEntropy(preds, labels, epsilon = 0.1, num_classes = 100):\n    one_hot = torch.zeros_like(preds).scatter(1, labels.view(-1, 1), 1)\n    one_hot = one_hot*(1-epsilon) + (1-one_hot)*epsilon/(num_classes-1)\n    log_prb = F.log_softmax(preds, dim=1)\n    loss = -(one_hot * log_prb).sum(dim=1)\n    return loss.mean()","execution_count":70,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Accuracy Evaluation</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_checkpoint = torch.load(final_path)\nnet = googlenet(mask = True)\nnet.load_state_dict(score_checkpoint['model_state_dict'])\nnet = net.cuda()\n\nloss_function = LabelSmoothedCrossEntropy\neval_training()","execution_count":71,"outputs":[{"output_type":"stream","text":"tensor(102, device='cuda:0')\ntensor(103, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(99, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(96, device='cuda:0')\ntensor(106, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(110, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(94, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(108, device='cuda:0')\ntensor(100, device='cuda:0')\ntensor(112, device='cuda:0')\ntensor(100, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(100, device='cuda:0')\ntensor(100, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(104, device='cuda:0')\ntensor(106, device='cuda:0')\ntensor(96, device='cuda:0')\ntensor(97, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(108, device='cuda:0')\ntensor(110, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(104, device='cuda:0')\ntensor(104, device='cuda:0')\ntensor(97, device='cuda:0')\ntensor(110, device='cuda:0')\ntensor(100, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(97, device='cuda:0')\ntensor(107, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(101, device='cuda:0')\ntensor(107, device='cuda:0')\ntensor(109, device='cuda:0')\ntensor(104, device='cuda:0')\ntensor(110, device='cuda:0')\ntensor(103, device='cuda:0')\ntensor(107, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(98, device='cuda:0')\ntensor(108, device='cuda:0')\ntensor(99, device='cuda:0')\ntensor(105, device='cuda:0')\ntensor(108, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(90, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(99, device='cuda:0')\ntensor(103, device='cuda:0')\ntensor(94, device='cuda:0')\ntensor(102, device='cuda:0')\ntensor(106, device='cuda:0')\ntensor(109, device='cuda:0')\ntensor(104, device='cuda:0')\ntensor(100, device='cuda:0')\ntensor(14, device='cuda:0')\nTest set: Average loss: 0.0023, Accuracy: 0.8018\n","name":"stdout"},{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"tensor(0.8018, device='cuda:0')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h3>Training method</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"resume_training = False\n#Set resume directory here if trained over multiple sessions\nmodel_path = ''\nif resume_training:\n    checkpoint = torch.load(model_path)","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = googlenet(mask = True)\nif resume_training:\n    net.load_state_dict(checkpoint['model_state_dict'])\nnet = net.cuda()\n\nloss_function = LabelSmoothedCrossEntropy\noptimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\nif resume_training:\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\ntrain_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.2) #learning rate decay\nif not resume_training:\n    iter_per_epoch = len(trainloader)\n    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * w)\n","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(1,101) # Building connections\n\nnum_prunings = 6\n\nfor i in range(num_prunings):   #Alternate conv and linear iterative pruning and retraining for two epochs\n    layer_type = 'Conv' if i%2==0 else 'Linear'\n    prune_by_std(layer_type)\n    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n    if i==num_prunings-1:\n        break\n    train(101+i*2,103+i*2)\n    \ntrain(101+i*2,151)  # Strengthening connections again\n\nfor i in range(num_prunings):   #Alternate conv and linear iterative pruning and retraining for two epochs\n    layer_type = 'Conv' if i%2==0 else 'Linear'\n    prune_by_std(layer_type)\n    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n    if i==num_prunings-1:\n        break\n    train(151+i*2,153+i*2)\n    \ntrain(151+i*2,201)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save({\n            'epoch': 200,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, 'sparsegnetv5_200ep.pth')\nprint('saved')","execution_count":75,"outputs":[{"output_type":"stream","text":"saved\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<pre>\n\n[1] 'Going Deeper with Convolutions', Szegedy et al. (2015)\n    <a href=\"https://arxiv.org/abs/1409.4842\">https://arxiv.org/abs/1409.4842</a>\n\n[2] 'Learning both Weights and Connections for Efficient Neural Networks', Han et al. (2015)\n    <a href=\"https://arxiv.org/abs/1506.02626\">https://arxiv.org/abs/1506.02626</a>\n\n[3] 'Improved Regularization of Convolutional Neural Networks with Cutout', T DeVries et al. (2017)\n    <a href=\"https://arxiv.org/abs/1708.04552\">https://arxiv.org/abs/1506.02626</a>\n</pre>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}